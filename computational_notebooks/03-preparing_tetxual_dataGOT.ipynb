{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pandas for working with dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#nltk libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem.porter import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create .TXT Files\n",
    "\n",
    "First, create large .txt files of each of the separate <em>Game of Thrones</em> and <em>The Legend of Korra</em> corpora. The reason for creating .txt files before doing word2vec or any kind of work with data is because I want to be able to easily access these files in other notebooks. This way, I do not have to re-run this process again.\n",
    "\n",
    "### Creating GoT Corpora\n",
    "I will create several different corpora: a <strong>cleaned</strong> and <strong>uncleaned</strong> version for each. This way, I can easily access the data when I need to. Here are the five GoT I'll be working with (ten since there will be the clean and unclean of each):\n",
    "- Seasons 1 & 2 \n",
    "- Seasons 3 & 4\n",
    "- Seasons 5 & 6\n",
    "- Season 7\n",
    "- Season 8\n",
    "\n",
    "I do not need to do this with TLoK data because I have already done this. Find more on my GitHub repository, [tracing fan uptakes in <em>The Legend of Korra</em>](https://github.com/caramessina/tracing_fan_uptakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading in the GoT CSV files\n",
    "gotmonth0 = pd.read_csv('data/group_month/got_1.csv')\n",
    "gotmonth1 = pd.read_csv('data/group_month/got_2.csv')\n",
    "gotmonth2 = pd.read_csv('data/group_month/got_3.csv')\n",
    "gotmonth3 = pd.read_csv('data/group_month/got_4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>additional tags</th>\n",
       "      <th>category</th>\n",
       "      <th>relationship</th>\n",
       "      <th>body</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-08</th>\n",
       "      <td>teen and up audiences,</td>\n",
       "      <td>incest, dreams,</td>\n",
       "      <td>m/m,</td>\n",
       "      <td>jon snow/robb stark,</td>\n",
       "      <td>up on the wall it is impossible to be warm and...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-02</th>\n",
       "      <td>general audiences,</td>\n",
       "      <td>possible incest,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the last time jon had seen his half-sister san...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-05</th>\n",
       "      <td>teen and up audiences,</td>\n",
       "      <td>tragedy, canonical character death, suicide, b...</td>\n",
       "      <td>f/m,</td>\n",
       "      <td>rhaegar targaryen/lyanna stark, robert barathe...</td>\n",
       "      <td>it is far too easy, to slip away. lyanna is kn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06</th>\n",
       "      <td>mature, teen and up audiences,</td>\n",
       "      <td>alternate universe, infidelity, unrequited lov...</td>\n",
       "      <td>f/m, f/m,</td>\n",
       "      <td>cersei lannister/oberyn martell, petyr baelish...</td>\n",
       "      <td>the vase shatters beautifully against the wall...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12</th>\n",
       "      <td>teen and up audiences, general audiences,</td>\n",
       "      <td>romance, action/adventure, incest, maleslash,</td>\n",
       "      <td>f/m, m/m,</td>\n",
       "      <td>brienne/jaime lannister, jaime lannister/rhaeg...</td>\n",
       "      <td>asshai is the end of the world. the valaryians...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             rating  \\\n",
       "month                                                 \n",
       "2006-08                     teen and up audiences,    \n",
       "2007-02                         general audiences,    \n",
       "2007-05                     teen and up audiences,    \n",
       "2007-06             mature, teen and up audiences,    \n",
       "2007-12  teen and up audiences, general audiences,    \n",
       "\n",
       "                                           additional tags    category  \\\n",
       "month                                                                    \n",
       "2006-08                                   incest, dreams,        m/m,    \n",
       "2007-02                                  possible incest,          NaN   \n",
       "2007-05  tragedy, canonical character death, suicide, b...       f/m,    \n",
       "2007-06  alternate universe, infidelity, unrequited lov...  f/m, f/m,    \n",
       "2007-12     romance, action/adventure, incest, maleslash,   f/m, m/m,    \n",
       "\n",
       "                                              relationship  \\\n",
       "month                                                        \n",
       "2006-08                              jon snow/robb stark,    \n",
       "2007-02                                                NaN   \n",
       "2007-05  rhaegar targaryen/lyanna stark, robert barathe...   \n",
       "2007-06  cersei lannister/oberyn martell, petyr baelish...   \n",
       "2007-12  brienne/jaime lannister, jaime lannister/rhaeg...   \n",
       "\n",
       "                                                      body  count  \n",
       "month                                                              \n",
       "2006-08  up on the wall it is impossible to be warm and...      1  \n",
       "2007-02  the last time jon had seen his half-sister san...      1  \n",
       "2007-05  it is far too easy, to slip away. lyanna is kn...      1  \n",
       "2007-06  the vase shatters beautifully against the wall...      2  \n",
       "2007-12  asshai is the end of the world. the valaryians...      2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the GoT CSV files\n",
    "got_all = pd.concat([gotmonth0, gotmonth1, gotmonth2, gotmonth3]).set_index('month')\n",
    "got_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>additional tags</th>\n",
       "      <th>category</th>\n",
       "      <th>relationship</th>\n",
       "      <th>body</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-04</th>\n",
       "      <td>teen and up audiences, not rated, general audi...</td>\n",
       "      <td>alternate universe - fantasy, game of thrones ...</td>\n",
       "      <td>m/m, f/m, f/m, multi, other, f/f, f/m, f/m, f/...</td>\n",
       "      <td>midoriya izuku/todoroki shouto, bakugou katsuk...</td>\n",
       "      <td>izuku midoriya is a green, curly-haired advent...</td>\n",
       "      <td>971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05</th>\n",
       "      <td>mature, teen and up audiences, not rated, expl...</td>\n",
       "      <td>game of thrones alternate universe, game of th...</td>\n",
       "      <td>f/f, f/m, m/m, f/m, f/f, f/m, m/m, f/f, multi,...</td>\n",
       "      <td>jon snow/daenerys targaryen, gilly (asoiaf)/sa...</td>\n",
       "      <td>\\n\\nprologue\\n\\n\\n\\n \\n\\n \\n\\n\\nharry was figh...</td>\n",
       "      <td>2498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    rating  \\\n",
       "month                                                        \n",
       "2019-04  teen and up audiences, not rated, general audi...   \n",
       "2019-05  mature, teen and up audiences, not rated, expl...   \n",
       "\n",
       "                                           additional tags  \\\n",
       "month                                                        \n",
       "2019-04  alternate universe - fantasy, game of thrones ...   \n",
       "2019-05  game of thrones alternate universe, game of th...   \n",
       "\n",
       "                                                  category  \\\n",
       "month                                                        \n",
       "2019-04  m/m, f/m, f/m, multi, other, f/f, f/m, f/m, f/...   \n",
       "2019-05  f/f, f/m, m/m, f/m, f/f, f/m, m/m, f/f, multi,...   \n",
       "\n",
       "                                              relationship  \\\n",
       "month                                                        \n",
       "2019-04  midoriya izuku/todoroki shouto, bakugou katsuk...   \n",
       "2019-05  jon snow/daenerys targaryen, gilly (asoiaf)/sa...   \n",
       "\n",
       "                                                      body  count  \n",
       "month                                                              \n",
       "2019-04  izuku midoriya is a green, curly-haired advent...    971  \n",
       "2019-05  \\n\\nprologue\\n\\n\\n\\n \\n\\n \\n\\n\\nharry was figh...   2498  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the 5 separate corpora based on the seasons\n",
    "got1_2 = got_all.loc['2006-08':'2013-02']\n",
    "got3_4 = got_all.loc['2013-03':'2015-03']\n",
    "got5_6 = got_all.loc['2015-07':'2017-06']\n",
    "got7 = got_all.loc['2017-07':'2019-03']\n",
    "got8 = got_all.loc['2019-04':'2019-09']\n",
    "\n",
    "got8.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the Corpora Dataframes into Text\n",
    "\n",
    "Since the dataframes are all confirmed to be within the proper timeframes, I will now use the functions to take the body of text from each published fanfiction, tokenize it, remove stopwords/punctuation, and stem it. Then, I will use the \"save_txt\" function to save each corpus as a text file so they can be used in the computational text analysis notebook.\n",
    "\n",
    "### Saving the Uncleaned Versions\n",
    "To save the unclean version, I will use the function 'column_to_txt,' which takes information from a column, converts it to a string, and then saves that string in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column_to_txt_file(dataframe,columnName,filePath):\n",
    "    '''\n",
    "    this function takes all the information from a specific column and joins it into a string. This is NOT for cleaning the text, just for saving the string.\n",
    "    Input: the name of the dataframe and the column name\n",
    "    Output: a string of all the words/characters from a column\n",
    "    '''\n",
    "    string = ' '.join(dataframe[columnName].tolist())\n",
    "    \n",
    "    #save the .txt\n",
    "    file = open(filePath, \"w\") \n",
    "    file.write(string)\n",
    "    file.close()\n",
    "    \n",
    "    print(string[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up on the wall it is impossible to be warm and the wind blows through your furs, regardless of their quality. the skies are grey and most of the black brothers are huddled within the relative shelter \n",
      "john didn't seem the kind of guy who likes reading. especially heavy literature, with complicated plots and lots of character deaths. however, john was pretty much the biggest fanboy when it came to a\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " part one: of trips and manuscripts  \n",
      "\n",
      "\n",
      "\n",
      "it is the wine and lobsters.\n",
      "\n",
      "\n",
      "\n",
      "sansa spends the entire trip pouring over tyrion's manuscript of his latest novel, only dropping it down for a good glas\n",
      "it is a truth universally acknowledged that a prince in possession of too many titles and a remarkable lack of responsibility must be in want of a pint. it was, therefore, no surprise that her path fi\n",
      "izuku midoriya is a green, curly-haired adventurer who does nothing but adventure. his mother bothers him numerous times about going out and how he is never careful. izuku, on the other hand, claims h\n"
     ]
    }
   ],
   "source": [
    "column_to_txt_file(got1_2,'body','data/group_month/got_all_txt/gotS1_2_unclean.txt')\n",
    "column_to_txt_file(got3_4,'body', 'data/group_month/got_all_txt/gotS3_4_unclean.txt')\n",
    "column_to_txt_file(got5_6,'body', 'data/group_month/got_all_txt/gotS5_6_unclean.txt')\n",
    "column_to_txt_file(got7,'body', 'data/group_month/got_all_txt/gotS7_unclean.txt')\n",
    "column_to_txt_file(got8,'body', 'data/group_month/got_all_txt/gotS8_unclean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Versions\n",
    "\n",
    "Next, I will \"clean\" each of the corpora and save them as the cleaned versions of the bodies of text. Here are the functions I will use:\n",
    "- \"column_to_token\" will take all the information in one column, make it into a string, and then tokenize it. \n",
    "\n",
    "- \"cleaning\" will lower all the capital letters, remove the stopwords, remove the punctuation, and stem the texts using NLTK's porter stemmer.\n",
    "\n",
    "- \"save_txt\"  takes a tokenized list, joins it to make it a string, and then saves that string in a desired location.\n",
    "\n",
    "So basically, I am taking all the text out of the \"body\" columns to make it a tokenized list, then clean that tokenized list using the \"cleaning\" function, rejoin the list to a string once it's cleaned, and save it. <strong>I will provide more detail for each step below</strong>\n",
    "\n",
    "Once I use the cleaning function to remove punctation, stopwords, and stem each word in the list, I will then join the tokenized list back to make it a string and save this new string as a .txt file so I may access it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def column_to_token(dataframe,columnName):\n",
    "    '''\n",
    "    this function takes all the information from a specific column, joins it to a string, and then tokenizes that string.\n",
    "    Input: the name of the dataframe and the column name\n",
    "    Output: a tokenized list of all the characters from that specific column\n",
    "    '''\n",
    "    string = ' '.join(dataframe[columnName].tolist())\n",
    "    corpus_token = word_tokenize(string)\n",
    "    return corpus_token \n",
    "    print(corpus_token[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords_ = ['``','`','--','...',\"'m\",\"''\",\"'re\",\"'ve\",'i','me', 'my',  'myself', \"“\", \"”\", 'we', 'our', '’', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her','hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', \"he's\",\"she's\",\"they're\",\"they've\",\"i've\"'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'do',\"n't\",\"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", \"'t\", \"'s\", 'wasn', \"wasn't\", 'weren', \"weren't\", \"would\", \"could\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    "def cleaning(token_text):\n",
    "    '''\n",
    "    This function takes a tokenized list of words/punctuation and does several \"cleaning\" methods\n",
    "    The first \"lowers\" everything, then it removes the stop words and punctuation. The stop words are listed and I used the punctuation list from NLTK (in my \"imports\")\n",
    "    Input: the tokenized list of the text\n",
    "    Output: the tokenized list of the text with all lower case, punctuation removed, and no stop words\n",
    "    '''\n",
    "    stemmer = PorterStemmer() \n",
    "    \n",
    "    text_lc = [word.lower() for word in token_text]\n",
    "    text_tokens_clean = [word for word in text_lc if word not in stopwords_]\n",
    "    text_tokens_clean_ = [word for word in text_tokens_clean if word not in string.punctuation]\n",
    "    tokens_stemmed = [stemmer.stem(w) for w in text_tokens_clean_]\n",
    "    return tokens_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_txt(cleanToken,filePath):\n",
    "    '''\n",
    "    take the tokenized list of words, convert to a string, and save it\n",
    "    input: the tokenized list of words and your new filepath\n",
    "    output: a saved file of the full corpus' string\n",
    "    '''\n",
    "    clean_string = \" \".join(cleanToken)\n",
    "    file2 = open(filePath,\"w\") \n",
    "    file2.write(clean_string)\n",
    "    file2.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Creating a Token\n",
    "\n",
    "The first function, column_to_token, takes all the characters from body and puts them into a string. A string is a a set of characters, almost like a bag of words. <strong>Strings</strong> usually appear between quotation marks, which is how the computer will recognize the words as a string. \"This is what a string might look like. I can have numbers, 2, in here, but the computer will see this as a long set of characters.\"\n",
    "\n",
    "I will then take my strings, which will just be all of the text from the \"body\" column, and tokenize them. <strong>Tokenizing</strong> takes a string and breaks it into a list based on whitespace (it's default). \"So a string like this\" is turned into ['a', 'string', 'like', 'this'] where each word between white space becomes an item in the list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up', 'on', 'the', 'wall', 'it']\n",
      "['part', 'one', ':', 'of', 'trips']\n"
     ]
    }
   ],
   "source": [
    "got1_2token = column_to_token(got1_2, 'body')\n",
    "got3_4token = column_to_token(got3_4, 'body')\n",
    "got5_6token = column_to_token(got5_6, 'body')\n",
    "got7token = column_to_token(got7, 'body')\n",
    "got8token = column_to_token(got8, 'body')\n",
    "\n",
    "print(got1_2token[:5])\n",
    "print(got5_6token[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Token\n",
    "\n",
    "Tokenizing will allow us to remove any unwanted items in the list. If, for example, I can  remove the word 'like' from the list, so my list will look like ['a', 'string', 'this'].\n",
    "\n",
    "Using the \"cleaning\" function, the code will go through each item in my list and remove it if it's a stop word, lowercase it if there are capital letters, remove any punctuation, and stem any items. <strong>Stemming</strong> is when you take the stem of a word that may have multiple endings, such as the word \"talk.\" Talk can appear as talk, talks, talked, talking, etc. A stemmer will recognize all these variations and turn all of them into \"talk.\" This way, the words are standardized.\n",
    "\n",
    "You will be able to see the difference between the output above and the output below. The output above still has the words 'up,' 'on,' and 'the,' which are all in the stopwords lists. The output below, however, has removed these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "got1_2clean = cleaning(got1_2token)\n",
    "got3_4clean = cleaning(got3_4token)\n",
    "got5_6clean = cleaning(got5_6token)\n",
    "got7clean = cleaning(got7token)\n",
    "got8clean = cleaning(got8token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(got1_2clean[:5])\n",
    "print(got5_6clean[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving The Resuls\n",
    "\n",
    "Now that there are five new, cleaned corpora, I will save each of them. The \"save_txt\" function takes a tokenized list, joins it back into a string, and saves the string as a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_txt(got1_2clean,'data/group_month/got_all_txt/gotS1_2_clean.txt')\n",
    "save_txt(got3_4clean,'data/group_month/got_all_txt/gotS3_4_clean.txt')\n",
    "save_txt(got5_6clean,'data/group_month/got_all_txt/gotS5_6_clean.txt')\n",
    "save_txt(got7clean,'data/group_month/got_all_txt/gotS7clean.txt')\n",
    "save_txt(got8clean,'data/group_month/got_all_txt/gotS8clean.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in the new files\n",
    "\n",
    "Now that I have created five corpora, each with the full body texts from different GoT fanfic seasons, I will now read them back in. This way, in case this notebook crashes, I can easily access the data again by just running the cell below. I mainly want to check that the read_txt function works and that both the clean and unclean texts run. I will use one corpus from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_txt(filePath):\n",
    "    '''\n",
    "    This function reads a file (specifically a text file) and tokenizes that file\n",
    "    Input: a .txt filepath of a string of words\n",
    "    Output: a tokenized list of words\n",
    "    '''\n",
    "    file = open(filePath, \"r\") \n",
    "    new_string = file.read() \n",
    "    file.close()\n",
    "    corpus_token = word_tokenize(new_string)\n",
    "    return corpus_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UNCLEANED\n",
    "gots_unclean_test = read_txt('data/group_month/got_all_txt/gotS1_2_unclean.txt')\n",
    "\n",
    "#CLEANED\n",
    "gots_clean_test = read_txt('data/group_month/got_all_txt/gotS1_2_clean.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['up', 'on', 'the', 'wall', 'it']\n",
      "['wall', 'imposs', 'warm', 'wind', 'blow']\n"
     ]
    }
   ],
   "source": [
    "print(gots_unclean_test[:5])\n",
    "print(gots_clean_test[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
